{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation is a statistical technique used to fill in missing or incomplete data points within a dataset. In real-world datasets, missing data is a common occurrence due to various reasons such as human error, equipment malfunction, or incomplete responses in surveys.\n",
    "\n",
    "Data imputation methods involve estimating the missing values based on the information available in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean/Median/Mode Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Case: Imagine you have a dataset containing information about customer ages, but some age values are missing. Mean, median, or mode imputation can be used to fill in these missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mean, median, or mode imputation can be useful in certain situations, particularly when dealing with missing data in numerical or categorical variables. Here's when you might consider using each method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When the data is normally distributed: Mean imputation works well when the data follows a normal distribution. In such cases, replacing missing values with the mean can preserve the overall distribution of the data.\n",
    "\n",
    "When the missing values are random or missing completely at random (MCAR): Mean imputation is often suitable when the missingness is random because it preserves the mean of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When the data is skewed or contains outliers: Median imputation is more robust than mean imputation to outliers and skewed distributions because it's less affected by extreme values.\n",
    "\n",
    "When the distribution is not normal: If the data is not normally distributed, using the median can be a better representation of the central tendency than the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode Imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When dealing with categorical variables: Mode imputation is appropriate for categorical variables where the data is represented by categories rather than numerical values.\n",
    "\n",
    "When the data has frequent values: Mode imputation can be useful when there are frequent or dominant values within a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss of variability**: Imputing missing values with the mean, median, or mode may reduce the variability of the dataset, leading to underestimation of standard errors and potentially biasing subsequent analyses.\n",
    "\n",
    "**Distortion of relationships**: Imputation with central tendency measures can distort relationships between variables, especially if missingness is related to the values of other variables.\n",
    "\n",
    "**Assumption of normality**: Mean imputation assumes that the data is normally distributed, which may not be valid in all cases.\n",
    "\n",
    "**Handling of categorical data**: Mean and median imputation are not suitable for categorical variables since they only work with numerical values. Mode imputation can be used for categorical data but may not capture the full complexity of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limitations to mean, median, or mode imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, mean, median, or mode imputation can be quick and simple methods for handling missing data, but they should be used judiciously, considering the characteristics of the data and the potential impact on subsequent analyses. It's always important to assess the appropriateness of imputation methods based on the specific context of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B     C\n",
      "0  1.0  5.0   NaN\n",
      "1  2.0  NaN  12.0\n",
      "2  NaN  7.0  13.0\n",
      "3  4.0  8.0  14.0\n",
      "4  5.0  9.0  15.0\n",
      "\n",
      "Mean-imputed DataFrame:\n",
      "     A     B     C\n",
      "0  1.0  5.00  13.5\n",
      "1  2.0  7.25  12.0\n",
      "2  3.0  7.00  13.0\n",
      "3  4.0  8.00  14.0\n",
      "4  5.0  9.00  15.0\n",
      "\n",
      "Median-imputed DataFrame:\n",
      "     A    B     C\n",
      "0  1.0  5.0  13.5\n",
      "1  2.0  7.5  12.0\n",
      "2  3.0  7.0  13.0\n",
      "3  4.0  8.0  14.0\n",
      "4  5.0  9.0  15.0\n",
      "\n",
      "Original DataFrame with categorical variable:\n",
      "       A\n",
      "0    red\n",
      "1   blue\n",
      "2    NaN\n",
      "3  green\n",
      "4   blue\n",
      "\n",
      "Mode-imputed DataFrame:\n",
      "       A\n",
      "0    red\n",
      "1   blue\n",
      "2   blue\n",
      "3  green\n",
      "4   blue\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, np.nan, 4, 5],\n",
    "        'B': [5, np.nan, 7, 8, 9],\n",
    "        'C': [np.nan, 12, 13, 14, 15]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Mean imputation\n",
    "mean_imputed_df = df.fillna(df.mean())\n",
    "print(\"\\nMean-imputed DataFrame:\")\n",
    "print(mean_imputed_df)\n",
    "\n",
    "# Median imputation\n",
    "median_imputed_df = df.fillna(df.median())\n",
    "print(\"\\nMedian-imputed DataFrame:\")\n",
    "print(median_imputed_df)\n",
    "\n",
    "# Mode imputation (for categorical variables)\n",
    "# Let's create a sample DataFrame with a categorical variable\n",
    "data = {'A': ['red', 'blue', np.nan, 'green', 'blue']}\n",
    "df_categorical = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nOriginal DataFrame with categorical variable:\")\n",
    "print(df_categorical)\n",
    "\n",
    "# Mode imputation\n",
    "mode_imputed_df = df_categorical.fillna(df_categorical.mode().iloc[0])\n",
    "print(\"\\nMode-imputed DataFrame:\")\n",
    "print(mode_imputed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (LOCF)Last observation carried forward : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Observation Carried Forward (LOCF) is a method of imputing missing values by carrying forward the last observed value to fill in the missing data points. It is commonly used in longitudinal or time-series data where observations are made at regular intervals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use LOCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Longitudinal Data**: LOCF is particularly useful in longitudinal studies where measurements are taken over time. In such studies, it's often reasonable to assume that the last observed value is a good approximation of the missing value, especially if the data is relatively stable over time.\n",
    "\n",
    "**Clinical Trials**: LOCF is frequently employed in clinical trials and medical research where subjects may drop out or miss follow-up visits. It provides a conservative approach to handling missing data, ensuring that the observed treatment effects are not underestimated.\n",
    "\n",
    "**Situations with Informative Dropout**: If missingness in the data is related to the outcome variable or other covariates, LOCF can help maintain the integrity of the data by preserving the observed trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of LOCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption of Continuity**: LOCF assumes that the last observed value remains valid and representative of the missing values until the next observation. However, this assumption may not hold true in all cases, especially if there are significant changes or fluctuations in the data between observations.\n",
    "\n",
    "**Potential Bias**: LOCF can introduce bias, particularly in studies with informative dropout patterns. If the reason for missingness is related to the outcome variable or other factors being studied, carrying forward the last observation may artificially inflate or deflate the observed trends.\n",
    "\n",
    "**Underestimation of Variability**: LOCF tends to underestimate the variability in the data since it essentially duplicates the last observed value for all missing points. This can lead to underestimation of standard errors and potentially affect the precision of statistical estimates.\n",
    "\n",
    "**Misrepresentation of Data**: LOCF may not accurately represent the true underlying trends in the data, especially if there are systematic changes or trends between observations. Imputing missing values with the last observed value may obscure important temporal patterns or variations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, while LOCF can be a convenient and conservative approach to handling missing data, especially in longitudinal studies with minimal missingness, it's essential to be aware of its limitations and potential biases. Researchers should carefully consider the appropriateness of LOCF in the context of their data and research questions, and explore alternative imputation methods if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B     C\n",
      "0  1.0  NaN  11.0\n",
      "1  NaN  7.0  12.0\n",
      "2  3.0  NaN   NaN\n",
      "3  NaN  9.0   NaN\n",
      "4  5.0  NaN  15.0\n",
      "\n",
      "LOCF-imputed DataFrame:\n",
      "     A    B     C\n",
      "0  1.0  NaN  11.0\n",
      "1  1.0  7.0  12.0\n",
      "2  3.0  7.0  12.0\n",
      "3  3.0  9.0  12.0\n",
      "4  5.0  9.0  15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HariharanSivakumar\\AppData\\Local\\Temp\\ipykernel_17620\\3789102125.py:15: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  locf_imputed_df = df.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {'A': [1, np.nan, 3, np.nan, 5],\n",
    "        'B': [np.nan, 7, np.nan, 9, np.nan],\n",
    "        'C': [11, 12, np.nan, np.nan, 15]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Perform LOCF imputation using fillna() with method='ffill'\n",
    "locf_imputed_df = df.fillna(method='ffill')\n",
    "print(\"\\nLOCF-imputed DataFrame:\")\n",
    "print(locf_imputed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors (KNN) imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors (KNN) imputation is a non-parametric method used to impute missing values in a dataset. It estimates missing values based on the values of their nearest neighbors in the feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works:\n",
    "\n",
    "Finding Nearest Neighbors: For each missing value, the algorithm identifies the k nearest neighbors (data points with the most similar features) with complete data.\n",
    "\n",
    "Imputation: The missing value is then imputed based on the values of its nearest neighbors. This can be done by taking the mean, median, or mode of the values of the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use KNN Imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complex Relationships**: KNN imputation is particularly useful when the relationships between variables are complex or nonlinear. Unlike simpler imputation methods like mean or median imputation, KNN can capture more intricate patterns in the data.\n",
    "\n",
    "**Missingness Mechanism**: KNN imputation can be effective when missingness is not completely random (MCAR) and there is some structure or pattern to the missing data. By leveraging information from similar data points, KNN can provide more accurate imputations in such cases.\n",
    "\n",
    "**High-Dimensional Data**: KNN imputation can handle high-dimensional datasets with many features. It's robust to the curse of dimensionality and can still perform well even when dealing with a large number of variables.\n",
    "\n",
    "**Small to Medium-Sized Datasets**: KNN imputation can be computationally expensive for large datasets due to the need to calculate distances between data points. It's more suitable for smaller to medium-sized datasets where computational resources are less constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of KNN Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computationally Intensive**: Calculating distances between data points can be computationally intensive, especially for large datasets or datasets with high dimensionality. This can make KNN imputation impractical for some applications.\n",
    "\n",
    "**Sensitive to Choice of k**: The performance of KNN imputation can be sensitive to the choice of the number of neighbors (k). Choosing an inappropriate value for k can lead to biased or inaccurate imputations.\n",
    "\n",
    "**Need for Preprocessing**: KNN imputation relies on the notion of distance between data points, so it's important to preprocess the data and scale the features appropriately to ensure that all variables contribute equally to the distance calculations.\n",
    "\n",
    "**Potential for Bias**: KNN imputation assumes that similar data points have similar missing values. However, this may not always hold true, especially in heterogeneous datasets where relationships between variables vary across different subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, KNN imputation can be a powerful tool for handling missing data, especially in cases where relationships between variables are complex and missingness is structured. However, researchers should be mindful of its computational requirements, sensitivity to parameter choices, and potential limitations when applying it to their datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B     C\n",
      "0  1.0  5.0   NaN\n",
      "1  2.0  NaN  12.0\n",
      "2  NaN  7.0  13.0\n",
      "3  4.0  8.0  14.0\n",
      "4  5.0  9.0  15.0\n",
      "\n",
      "KNN-imputed DataFrame:\n",
      "     A    B     C\n",
      "0  1.0  5.0  12.5\n",
      "1  2.0  6.0  12.0\n",
      "2  3.0  7.0  13.0\n",
      "3  4.0  8.0  14.0\n",
      "4  5.0  9.0  15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, np.nan, 4, 5],\n",
    "        'B': [5, np.nan, 7, 8, 9],\n",
    "        'C': [np.nan, 12, 13, 14, 15]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize KNNImputer with k=2\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Perform KNN imputation\n",
    "knn_imputed = imputer.fit_transform(df)\n",
    "\n",
    "# Convert the imputed array back to a DataFrame\n",
    "knn_imputed_df = pd.DataFrame(knn_imputed, columns=df.columns)\n",
    "\n",
    "print(\"\\nKNN-imputed DataFrame:\")\n",
    "print(knn_imputed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always neccessary to perform data imputation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it's not always necessary to perform data imputation. Whether or not data imputation is necessary depends on various factors including the nature of the missing data, the goals of the analysis, and the potential impact of missing data on the validity and interpretability of the results. Here are some scenarios where data imputation may not be necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimal Missingness**: If the dataset has very few missing values and the missingness is negligible in relation to the overall dataset, imputation may not be necessary. In such cases, it may be feasible to simply exclude observations with missing values from the analysis without significantly compromising the validity of the results.\n",
    "\n",
    "**Missing Completely at Random (MCAR)**: If the missing data can be assumed to be missing completely at random (MCAR), meaning that the probability of missingness is unrelated to the observed or unobserved data, then the missingness is ignorable and may not require imputation. However, it's essential to assess the randomness of missingness using appropriate statistical tests.\n",
    "\n",
    "**Complete Case Analysis**: In some cases, complete case analysis, where only observations with complete data are included in the analysis, may be sufficient and appropriate. This approach avoids the need for imputation but may result in a reduction in sample size and statistical power.\n",
    "\n",
    "**Sensitive Analyses**: In sensitive analyses or situations where imputation may introduce bias or uncertainty, such as in causal inference studies or when dealing with high-dimensional data, researchers may opt to perform sensitivity analyses to assess the robustness of their results to different missing data handling strategies, including imputation.\n",
    "\n",
    "**Exploratory Data Analysis**: During exploratory data analysis or preliminary investigations, researchers may choose to explore the patterns and characteristics of missing data before deciding whether imputation is necessary. Understanding the nature and mechanisms of missingness can inform the choice of appropriate imputation methods or alternative strategies for handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, while data imputation can be a useful technique for handling missing data, it's not always necessary or appropriate. Researchers should carefully consider the characteristics of the dataset, the assumptions underlying the missing data mechanism, and the implications of missing data on the validity and interpretation of their results before deciding whether to perform imputation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
